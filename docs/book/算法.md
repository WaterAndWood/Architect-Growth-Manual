1	算法
1.1	加密算法
1.1.1	AES 
高级加密标准(AES,Advanced Encryption Standard)为最常见的对称加密算法(微信小程序加密传输就是用这个加密算法的)。对称加密算法也就是加密和解密用相同的密钥，具体的加密流程如下图： 
<div align=center>

![1589093380174.png](..\images\1589093380174.png)

</div>

1.1.1		RSA 
RSA 加密算法是一种典型的非对称加密算法，它基于大数的因式分解数学难题，它也是应用最广泛的非对称加密算法。 
非对称加密是通过两个密钥（公钥-私钥）来实现对数据的加密和解密的。公钥用于加密，私钥用于解密。 

<div align=center>

![1589093412160.png](..\images\1589093412160.png)

</div>

<div align=center>

![1589093431138.png](..\images\1589093431138.png)

</div>

1.1.1	CRC 
循环冗余校验(Cyclic Redundancy Check, CRC)是一种根据网络数据包或电脑文件等数据产生简短固定位数校验码的一种散列函数，主要用来检测或校验数据传输或者保存后可能出现的错误。
它是利用除法及余数的原理来作错误侦测的。 
1.1.2	MD5 
  MD5 常常作为文件的签名出现，我们在下载文件的时候，常常会看到文件页面上附带一个扩展名为.MD5 的文本或者一行字符，这行字符就是就是把整个文件当作原数据通过 MD5 计算后的值，我们下载文件后，可以用检查文件 MD5 信息的软件对下载到的文件在进行一次计算。两次结果对比就可以确保下载到文件的准确性。 另一种常见用途就是网站敏感信息加密，比如用户名密码，支付签名等等。随着 https 技术的普及，现在的网站广泛采用前台明文传输到后台，MD5 加密
（使用偏移量）的方式保护敏感数据保护站点和数据安全。 
1.1	一致性算法 
1.1.1		Paxos 
Paxos 算法解决的问题是一个分布式系统如何就某个值（决议）达成一致。一个典型的场景是，在一个分布式数据库系统中，如果各节点的初始状态一致，每个节点执行相同的操作序列，那么他们最后能得到一个一致的状态。为保证每个节点执行相同的命令序列，需要在每一条指令上执行一个“一致性算法”以保证每个节点看到的指令一致。zookeeper 使用的 zab 算法是该算法的一个实现。 在 Paxos 算法中，有三种角色：Proposer，Acceptor，Learners 
Paxos 三种角色：Proposer，Acceptor，Learners 
Proposer： 
只要 Proposer 发的提案被半数以上 Acceptor 接受，Proposer 就认为该提案里的 value 被选定了。 
Acceptor： 
只要 Acceptor 接受了某个提案，Acceptor 就认为该提案里的 value 被选定了。 
Learner： 
Acceptor 告诉 Learner 哪个 value 被选定，Learner 就认为那个 value 被选定。 
Paxos 算法分为两个阶段。具体如下： 
阶段一（准leader确定 ）： 
(a) Proposer选择一个提案编号N，然后向半数以上的Acceptor发送编号为N的Prepare请求。 (b) 如果一个 Acceptor 收到一个编号为 N 的 Prepare 请求，且 N 大于该 Acceptor 已经响应过的所有 Prepare 请求的编号，那么它就会将它已经接受过的编号最大的提案（如果有的话）作为响应反馈给 Proposer，同时该 Acceptor 承诺不再接受任何编号小于 N 的提案。 
阶段二（leader确认）： 
(a)	如果 Proposer 收到半数以上 Acceptor 对其发出的编号为 N 的 Prepare 请求的响应，那么它就会发送一个针对[N,V]提案的 Accept 请求给半数以上的 Acceptor。注意：V 就是收到的响应中编号最大的提案的 value，如果响应中不包含任何提案，那么 V 就由 Proposer 自己决定。 
(b)	如果 Acceptor 收到一个针对编号为 N 的提案的 Accept 请求，只要该 Acceptor 没有对编号大于 N 的 Prepare 请求做出过响应，它就接受该提案。 
1.1.2	Zab 
   
  ZAB( ZooKeeper Atomic Broadcast , ZooKeeper 原子消息广播协议）协议包括两种基本的模式：崩溃恢复和消息广播 
1.	当整个服务框架在启动过程中，或是当 Leader 服务器出现网络中断崩溃退出与重启等异常情况时，ZAB 就会进入恢复模式并选举产生新的 Leader 服务器。 
2.	当选举产生了新的 Leader 服务器，同时集群中已经有过半的机器与该 Leader 服务器完成了状态同步之后，ZAB 协议就会退出崩溃恢复模式，进入消息广播模式。 
3.	当有新的服务器加入到集群中去，如果此时集群中已经存在一个 Leader 服务器在负责进行消息广播，那么新加入的服务器会自动进入数据恢复模式，找到 Leader 服务器，并与其进行数据同步，然后一起参与到消息广播流程中去。 
以上其实大致经历了三个步骤： 
1.崩溃恢复：主要就是Leader 选举过程 
2.数据同步：Leader服务器与其他服务器进行数据同步 
3.消息广播：Leader服务器将数据发送给其他服务器说明：zookeeper 章节对该协议有详细描述。 
1.1.3	Raft 
与 Paxos 不同 Raft 强调的是易懂（Understandability），Raft 和 Paxos 一样只要保证 n/2+1 节点正常就能够提供服务；raft 把算法流程分为三个子问题：选举（Leader election）、日志复制
（Log replication）、安全性（Safety）三个子问题。 
1.1.3.1	角色 
  Raft 把集群中的节点分为三种状态：Leader、 Follower 、Candidate，理所当然每种状态负责的任务也是不一样的，Raft 运行时提供服务的时候只存在 Leader 与 Follower 两种状态； 
Leader（领导者-日志管理） 
负责日志的同步管理，处理来自客户端的请求，与 Follower 保持这 heartBeat 的联系； 
Follower（追随者-日志同步） 
刚启动时所有节点为Follower状态，响应Leader的日志同步请求，响应Candidate的请求，把请求到 Follower 的事务转发给 Leader； 
Candidate（候选者-负责选票） 
负责选举投票，Raft 刚启动时由一个节点从 Follower 转为 Candidate 发起选举，选举出
Leader 后从 Candidate 转为 Leader 状态； 
1.1.3.2	Term（任期） 
  在 Raft 中使用了一个可以理解为周期（第几届、任期）的概念，用 Term 作为一个周期，每个 Term 都是一个连续递增的编号，每一轮选举都是一个 Term 周期，在一个 Term 中只能产生一个 Leader；当某节点收到的请求中 Term 比当前 Term 小时则拒绝该请求。 
1.1.3.3	选举（Election） 
选举定时器 
  Raft 的选举由定时器来触发，每个节点的选举定时器时间都是不一样的，开始时状态都为 Follower 某个节点定时器触发选举后 Term 递增，状态由 Follower 转为 Candidate，向其他节点发起 RequestVote RPC 请求，这时候有三种可能的情况发生： 
  1：该RequestVote请求接收到n/2+1（过半数）个节点的投票，从Candidate 转为Leader，向其他节点发送 heartBeat 以保持 Leader 的正常运转。 
  2：在此期间如果收到其他节点发送过来的 AppendEntries RPC 请求，如该节点的 Term 大则当前节点转为 Follower，否则保持 Candidate 拒绝该请求。 
  3：Election timeout 发生则 Term 递增，重新发起选举 
  在一个 Term 期间每个节点只能投票一次，所以当有多个 Candidate 存在时就会出现每个 Candidate 发起的选举都存在接收到的投票数都不过半的问题，这时每个 Candidate 都将 Term 递增、重启定时器并重新发起选举，由于每个节点中定时器的时间都是随机的，所以就不会多次存在有多个 Candidate 同时发起投票的问题。 
  在 Raft 中当接收到客户端的日志（事务请求）后先把该日志追加到本地的 Log 中，然后通过 heartbeat 把该 Entry 同步给其他 Follower，Follower 接收到日志后记录日志然后向 Leader 发送 ACK，当 Leader 收到大多数（n/2+1）Follower 的 ACK 信息后将该日志设置为已提交并追加到本地磁盘中，通知客户端并在下个 heartbeat 中 Leader 将通知所有的 Follower 将该日志存储在自己的本地磁盘中。 
1.1.3.4	安全性（Safety） 
安全性是用于保证每个节点都执行相同序列的安全机制如当某个 Follower 在当前 Leader commit Log 时变得不可用了，稍后可能该 Follower 又会倍选举为 Leader，这时新 Leader 可能会用新的 Log 覆盖先前已 committed 的 Log，这就是导致节点执行不同序列；Safety 就是用于保证选举出来的 Leader 一定包含先前 commited Log 的机制； 
  选举安全性（Election Safety）：每个 Term 只能选举出一个 Leader 
Leader 完整性（Leader Completeness）：这里所说的完整性是指 Leader 日志的完整性， Raft 在选举阶段就使用 Term 的判断用于保证完整性：当请求投票的该 Candidate 的 Term 较大或 Term 相同 Index 更大则投票，该节点将容易变成 leader。 
1.1.3.5	raft 协议和 zab 协议区别相同点 
	采用 quorum 来确定整个系统的一致性,这个 quorum 一般实现是集群中半数以上的服务器, 
	zookeeper 里还提供了带权重的 quorum 实现. 
	都由 leader 来发起写操作. 
	都采用心跳检测存活性 
	leader election 都采用先到先得的投票方式不同点 
	zab 用的是 epoch 和 count 的组合来唯一表示一个值, 而 raft 用的是 term 和 index 
	zab 的 follower 在投票给一个 leader 之前必须和 leader 的日志达成一致,而 raft 的 follower 则简单地说是谁的 term 高就投票给谁 
	raft 协议的心跳是从 leader 到 follower, 而 zab 协议则相反 
	raft 协议数据只有单向地从 leader 到 follower(成为 leader 的条件之一就是拥有最新的 log),  
而 zab 协议在 discovery 阶段, 一个 prospective leader 需要将自己的 log 更新为 quorum 里面最新的 log,然后才好在 synchronization 阶段将 quorum 里的其他机器的 log 都同步到一致. 
1.1.4		NWR 
N：在分布式存储系统中，有多少份备份数据 
W：代表一次成功的更新操作要求至少有w份数据写入成功 
R： 代表一次成功的读数据操作要求至少有R份数据成功读取 
NWR值的不同组合会产生不同的一致性效果，当W+R>N的时候，整个系统对于客户端来讲能保
证强一致性。而如果 R+W<=N，则无法保证数据的强一致性。以常见的 N=3、W=2、R=2 为例： 
N=3，表示，任何一个对象都必须有三个副本（Replica），W=2 表示，对数据的修改操作（Write）只需要在 3 个 Replica 中的 2 个上面完成就返回，R=2 表示，从三个对象中要读取到 2 个数据对象，才能返回。 
<div align=center>

![1589093567283.png](..\images\1589093567283.png)

</div>

1.1.1	Gossip 
Gossip 算法又被称为反熵（Anti-Entropy），熵是物理学上的一个概念，代表杂乱无章，而反熵就是在杂乱无章中寻求一致，这充分说明了 Gossip 的特点：在一个有界网络中，每个节点都随机地与其他节点通信，经过一番杂乱无章的通信，最终所有节点的状态都会达成一致。每个节点可能知道所有其他节点，也可能仅知道几个邻居节点，只要这些节可以通过网络连通，最终他们的状态都是一致的，当然这也是疫情传播的特点。 
1.1.2	一致性 Hash 
 一致性哈希算法(Consistent Hashing Algorithm)是一种分布式算法，常用于负载均衡。 Memcached client 也选择这种算法，解决将 key-value 均匀分配到众多 Memcached server 上的问题。它可以取代传统的取模操作，解决了取模操作无法应对增删 Memcached Server 的问题 (增删 server 会导致同一个 key,在 get 操作时分配不到数据真正存储的 server，命中率会急剧下
降)。 
1.1.2.1	一致性 Hash 特性 
	平衡性(Balance)：平衡性是指哈希的结果能够尽可能分布到所有的缓冲中去，这样可以使得所有的缓冲空间都得到利用。 
	单调性(Monotonicity)：单调性是指如果已经有一些内容通过哈希分派到了相应的缓冲中，又有新的缓冲加入到系统中。哈希的结果应能够保证原有已分配的内容可以被映射到新的缓冲中去，而不会被映射到旧的缓冲集合中的其他缓冲区。容易看到，上面的简单求余算法 hash(object)%N 难以满足单调性要求。 
	平滑性(Smoothness)：平滑性是指缓存服务器的数目平滑改变和缓存对象的平滑改变是一致的。 
1.1.2.2	一致性 Hash 原理 
1.建构环形hash 空间： 
1.	考虑通常的 hash 算法都是将 value 映射到一个 32 为的 key 值，也即是 0~2^32-1 次方的数值空间；我们可以将这个空间想象成一个首（ 0 ）尾（ 2^32-1 ）相接的圆环。 
2.把需要缓存的内容(对象)映射到hash 空间 
2.	接下来考虑 4 个对象 object1~object4 ，通过 hash 函数计算出的 hash 值 key 在环上的分布 
3.把服务器(节点)映射到hash 空间 
3.	Consistent hashing 的基本思想就是将对象和 cache 都映射到同一个 hash 数值空间中，并且使用相同的 hash 算法。一般的方法可以使用 服务器(节点) 机器的 IP 地址或者机器名作为 hash 输入。 
4.把对象映射到服务节点 
4.	现在服务节点和对象都已经通过同一个 hash 算法映射到 hash 数值空间中了，首先确定对象 hash 值在环上的位置，从此位置沿环顺时针“行走”，第一台遇到的服务器就是其应该定位到的服务器。 
<div align=center>

![1589093603949.png](..\images\1589093603949.png)

</div>

考察cache 的变动 
1.	通过 hash 然后求余的方法带来的最大问题就在于不能满足单调性，当 cache 有所变动时， cache 会失效。 
1.1	移除 cache：考虑假设 cache B 挂掉了，根据上面讲到的映射方法，这时受影响的将仅是那些沿 cache B 逆时针遍历直到下一个 cache （ cache C ）之间的对象。 
1.2	添加 cache：再考虑添加一台新的 cache D 的情况，这时受影响的将仅是那些沿 cache D 逆时针遍历直到下一个 cache 之间的对象，将这些对象重新映射到 cache D 上即可。 
虚拟节点 
hash 算法并不是保证绝对的平衡，如果 cache 较少的话，对象并不能被均匀的映射到 cache 上，为了解决这种情况， consistent hashing 引入了“虚拟节点”的概念，它可以如下定义：虚拟节点（ virtual node ）是实际节点在 hash 空间的复制品（ replica ），一实际个节点对应了若干个“虚拟节点”，这个对应个数也成为“复制个数”，“虚拟节点”在 hash 空间中以 hash 值排列。 
仍以仅部署 cache A 和 cache C 的情况为例。现在我们引入虚拟节点，并设置“复制个数”为 2 ，这就意味着一共会存在 4 个“虚拟节点”， cache A1, cache A2 代表了 cache A； cache C1, cache C2 代表了 cache C 。此时，对象到“虚拟节点”的映射关系为： 
objec1->cache A2 ； objec2->cache A1 ； objec3->cache C1 ； objec4->cache C2 ；因此对象 object1 和 object2 都被映射到了 cache A 上，而 object3 和 object4 映射到了 cache 
C 上；平衡性有了很大提高。 
引入“虚拟节点”后，映射关系就从 { 对象 -> 节点 } 转换到了 { 对象 -> 虚拟节点 } 。查询物体所在 cache 时的映射关系如下图 所示。 
<div align=center>

![1589093632076.png](..\images\1589093632076.png)

</div>


1.1	Bloom Filter
布隆过滤器(BloomFilter)的原理、实现和探究
1.1.1	背景
如果想判断一个元素是不是在一个集合里，一般想到的是将集合中所有元素保存起来，然后通过比较确定。链表、树、散列表（又叫哈希表，Hash table）等等数据结构都是这种思路，存储位置要么是磁盘，要么是内存。很多时候要么是以时间换空间，要么是以空间换时间。
在响应时间要求比较严格的情况下，如果我们存在内里，那么随着集合中元素的增加，我们需要的存储空间越来越大，以及检索的时间越来越长，导致内存开销太大、时间效率变低。
此时需要考虑解决的问题就是，在数据量比较大的情况下，既满足时间要求，又满足空间的要求。即我们需要一个时间和空间消耗都比较小的数据结构和算法。Bloom Filter就是一种解决方案。
1.1.2	概念
布隆过滤器（英语：Bloom Filter）是1970年由布隆提出的。它实际上是一个很长的二进制向量和一系列随机映射函数。布隆过滤器可以用于检索一个元素是否在一个集合中。它的优点是空间效率和查询时间都远远超过一般的算法，缺点是有一定的误识别率和删除困难。
1.1.3	原理
布隆过滤器的原理是，当一个元素被加入集合时，通过K个散列函数将这个元素映射成一个位数组中的K个点，把它们置为1。检索时，我们只要看看这些点是不是都是1就（大约）知道集合中有没有它了：如果这些点有任何一个0，则被检元素一定不在；如果都是1，则被检元素很可能在。这就是布隆过滤器的基本思想。
Bloom Filter跟单哈希函数Bit-Map不同之处在于：Bloom Filter使用了k个哈希函数，每个字符串跟k个bit对应。从而降低了冲突的概率。
<div align=center>

![1589094551971.png](..\images\1589094551971.png)

</div>

14.19.1	缺点
bloom filter之所以能做到在时间和空间上的效率比较高，是因为牺牲了判断的准确率、删除的便利性
•	存在误判，可能要查到的元素并没有在容器中，但是hash之后得到的k个位置上值都是1。如果bloom filter中存储的是黑名单，那么可以通过建立一个白名单来存储可能会误判的元素。
•	删除困难。一个放入容器的元素映射到bit数组的k个位置上是1，删除的时候不能简单的直接置为0，可能会影响其他元素的判断。可以采用Counting Bloom Filter

14.19.2	实现
布隆过滤器有许多实现与优化，Guava中就提供了一种Bloom Filter的实现。
	在使用bloom filter时，绕不过的两点是预估数据量n以及期望的误判率fpp，
	在实现bloom filter时，绕不过的两点就是hash函数的选取以及bit数组的大小。

对于一个确定的场景，我们预估要存的数据量为n，期望的误判率为fpp，然后需要计算我们需要的Bit数组的大小m，以及hash函数的个数k，并选择hash函数
14.19.2.1	Bit数组大小选择 
根据预估数据量n以及误判率fpp，bit数组大小的m的计算方式： 
<div align=center>

![1589094591649.png](..\images\1589094591649.png)

</div>

1.1.1.1.1	com.google.common.hash.BloomFilter源码
/**
 * 计算 Bloom Filter的bit位数m
 *
 * <p>See http://en.wikipedia.org/wiki/Bloom_filter#Probability_of_false_positives for the
 * formula.
 *
 * @param n 预期数据量
 * @param p 误判率 (must be 0 < p < 1)
 */ 
@VisibleForTesting 
static long optimalNumOfBits(long n, double p) { 
  if (p == 0) { 
    p = Double.MIN_VALUE; 
  } 
  return (long) (-n * Math.log(p) / (Math.log(2) * Math.log(2))); 
} 

/**
 * 计算最佳k值，即在Bloom过滤器中插入的每个元素的哈希数
 *
 * <p>See http://en.wikipedia.org/wiki/File:Bloom_filter_fp_probability.svg for the formula.
 *
 * @param n 预期数据量
 * @param m bloom filter中总的bit位数 (must be positive)
 */ 
@VisibleForTesting 
static int optimalNumOfHashFunctions(long n, long m) { 
  // (m / n) * log(2), but avoid truncation due to division! 
  return Math.max(1, (int) Math.round((double) m / n * Math.log(2))); 
} 


1.1.1.2	哈希函数选择
由预估数据量n以及bit数组长度m，可以得到一个hash函数的个数k：
<div align=center>

![1589094678057.png](..\images\1589094678057.png)

</div>
哈希函数的选择对性能的影响应该是很大的，一个好的哈希函数要能近似等概率的将字符串映射到各个Bit。选择k个不同的哈希函数比较麻烦，一种简单的方法是选择一个哈希函数，然后送入k个不同的参数。
哈希函数个数k、位数组大小m、加入的字符串数量n的关系可以参考Bloom Filters - the math，Bloom_filter-wikipedia


BloomFilter实现的另一个重点就是怎么利用hash函数把数据映射到bit数组中。Guava的实现是对元素通过MurmurHash3计算hash值，将得到的hash值取高8个字节以及低8个字节进行计算，以得当前元素在bit数组中对应的多个位置。MurmurHash3算法详见:Murmur哈希，于2008年被发明。这个算法hbase,redis,kafka都在使用。

这个过程的实现在两个地方：
•	将数据放入bloom filter中
•	判断数据是否已在bloom filter中
这两个地方的实现大同小异，区别只是，前者是put数据，后者是查数据。
 
这里看一下put的过程，hash策略以MURMUR128_MITZ_64为例：

public <T> boolean put( 
    T object, Funnel<? super T> funnel, int numHashFunctions, LockFreeBitArray bits) { 
  long bitSize = bits.bitSize(); 
    
  //利用MurmurHash3得到数据的hash值对应的字节数组 
  byte[] bytes = Hashing.murmur3_128().hashObject(object, funnel).getBytesInternal(); 
    
    
  //取低8个字节、高8个字节，转成long类型 
  long hash1 = lowerEight(bytes); 
  long hash2 = upperEight(bytes); 
    
  boolean bitsChanged = false; 
    
    
  //这里的combinedHash = hash1 + i * hash2 
  long combinedHash = hash1; 
    
    
  //根据combinedHash，得到放入的元素在bit数组中的k个位置，将其置1 
  for (int i = 0; i < numHashFunctions; i++) { 
    bitsChanged |= bits.set((combinedHash & Long.MAX_VALUE) % bitSize); 
    combinedHash += hash2; 
  } 
  return bitsChanged; 
} 


 



1.1.1	使用
public class BloomFilterTest { 
        
    public static void main(String[] args) { 
        long expectedInsertions = 10000000; 
        double fpp = 0.00001; 
    
        BloomFilter<CharSequence> bloomFilter = BloomFilter.create(Funnels.stringFunnel(Charsets.UTF_8), expectedInsertions, fpp); 
    
        bloomFilter.put("aaa"); 
        bloomFilter.put("bbb"); 
        boolean containsString = bloomFilter.mightContain("aaa"); 
        System.out.println(containsString); 
    
        BloomFilter<Email> emailBloomFilter = BloomFilter 
                .create((Funnel<Email>) (from, into) -> into.putString(from.getDomain(), Charsets.UTF_8), 
                        expectedInsertions, fpp); 
    
        emailBloomFilter.put(new Email("sage.wang", "quanr.com")); 
        boolean containsEmail = emailBloomFilter.mightContain(new Email("sage.wangaaa", "quanr.com")); 
        System.out.println(containsEmail); 
    } 
    
    @Data 
    @Builder 
    @ToString 
    @AllArgsConstructor 
    public static class Email { 
        private String userName; 
        private String domain; 
    } 
    
} 

1.1.2	应用
常见的几个应用场景：
•	cerberus在收集监控数据的时候, 有的系统的监控项量会很大, 需要检查一个监控项的名字是否已经被记录到db过了, 如果没有的话就需要写入db.
•	爬虫过滤已抓到的url就不再抓，可用bloom filter过滤
•	垃圾邮件过滤。如果用哈希表，每存储一亿个 email地址，就需要 1.6GB的内存（用哈希表实现的具体办法是将每一个 email地址对应成一个八字节的信息指纹，然后将这些信息指纹存入哈希表，由于哈希表的存储效率一般只有 50%，因此一个 email地址需要占用十六个字节。一亿个地址大约要 1.6GB，即十六亿字节的内存）。因此存贮几十亿个邮件地址可能需要上百 GB的内存。而Bloom Filter只需要哈希表 1/8到 1/4 的大小就能解决同样的问题。
1.2	泊松分布
Poisson分布，是一种统计与概率学里常见到的离散概率分布，由法国数学家西莫恩·德尼·泊松（Siméon-Denis Poisson）在1838年时发表。适合用于描述单位时间（单位面积）内随机事件发生的次数（个数），例如：一分钟内不断抛硬币并得到正面向上的次数为 30 次，求解得到正面向上次数为 50 次的概率是多少。
1.2.1	概率密度函数
<div align=center>

![1589094744714.png](..\images\1589094744714.png)

</div>


	λ 表示单位时间（单位面积）内随机事件的平均发生率
	k! 表示 k 的阶乘并且 k 取非负整数
<div align=center>

![1589094771607.png](..\images\1589094771607.png)

</div>

1.1.1	满足的条件
1. 这个事件是一个小概率事件。
2. 事件的每次发生都是独立的。
3. 事件的概率是稳定的。
例子：公交车站有很多不同线路的公交车，平均每 5 分钟会来两辆公交车，求解 5 分钟内来 5 辆公交车的概率有多大？
解析：在这里问题里，随机事件（公交车来到）拥有固定的平均概率（每 5 钟 2 辆），每次事件随机且独立（不同的线路），所以这个随机事件在单位时间（5 分钟）内出现的次数就近似的服从语泊松分布。
最终代入公式求解得：P(X=k=5) = (2^5 / 5!) * e^(-2) 概率约为 0.0361
除此之外，一放射性源放射出的 alfa 粒子数，某电话交换台收到的电话呼叫数，到某机场降落的飞机数，一个售货员接待的顾客数，一台纺纱机的断头数，都服从泊松分布。
1.1.2	泊松部分函数密度(累积函数)曲线
<div align=center>

![1589094800715.png](..\images\1589094800715.png)

</div>


1.1.1	分布曲线特点
当 k 小于 λ 时，k 每增加 1 累积概率增速会逐渐加大
而当 k 大于 λ 后，累积概率的降速也会逐渐加大。
1.1.2	为什么说现实生活多数服从泊松分布？
假定一个事件在一段时间内随机发生，且符合以下条件：
（1）将该时间段无限分隔成若干个小的时间段，在这个接近于零的小时间段里，该事件发生一次的概率与这个极小时间段的长度成正比。
（2）在每一个极小时间段内，该事件发生两次及以上的概率恒等于零。
（3）该事件在不同的小时间段里，发生与否相互独立。 则该事件称为 Poisson Process。

医院的例子，如果把一天分成 24 个小时，或 24*60 分，或 24*3600秒。时间区间越分越短，该区间内来病人的概率就越小（比如医院在 12 点到 12 点又一毫秒之间来病人的概率是不是很接近于零？），符合条件（1）；另外如果我们把时间分的很细很细，是不是同时来两个病人（或者两个以上的病人）就是不可能的事件？符合条件（2）；条件（3）的要求则比较苛刻，要求病人们来医院的概率必须是相互独立的，如果不然，则事件不能看作是泊松分布。
所以，简单来说，大多数现实生活中的例子中如果事件是相互独立的，那么它就很可能是符合泊松分布的。
1.1.3	场景
	JDK中的hashmap在存储数据时定义链表转换成红黑树的阈值（8）

1.1	二分查找 
又叫折半查找，要求待查找的序列有序。每次取中间位置的值与待查关键字比较，如果中间位置的值比待查关键字大，则在前半部分循环这个查找的过程，如果中间位置的值比待查关键字小，则在后半部分循环这个查找的过程。直到查找到了为止，否则序列中没有待查的关键字。 
  public static int biSearch(int []array,int a){         int lo=0;         int hi=array.length-1;         int mid;         while(lo<=hi){             mid=(lo+hi)/2;//中间位置             if(array[mid]==a){                 return mid+1; 
            }else if(array[mid]<a){ //向右查找                 lo=mid+1;             }else{ //向左查找                 hi=mid-1; 
            } 
        }         return -1; 
    } 

1.1	冒泡排序算法 
（1）	比较前后相邻的二个数据，如果前面数据大于后面的数据，就将这二个数据交换。 
（2）	这样对数组的第 0 个数据到 N-1 个数据进行一次遍历后，最大的一个数据就“沉”到数组第
N-1 个位置。 
（3）	N=N-1，如果 N 不为 0 就重复前面二步，否则排序完成。 
  public static void bubbleSort1(int [] a, int n){     int i, j; 
 
21.1.3.	插入排序算法 
通过构建有序序列，对于未排序数据，在已排序序列中从后向前扫描，找到相应的位置并插入。插入排序非常类似于整扑克牌。在开始摸牌时，左手是空的，牌面朝下放在桌上。接着，一次从桌上摸起一张牌，并将它插入到左手一把牌中的正确位置上。为了找到这张牌的正确位置，要将它与手中已有的牌从右到左地进行比较。无论什么时候，左手中的牌都是排好序的。 
如果输入数组已经是排好序的话，插入排序出现最佳情况，其运行时间是输入规模的一个线性函数。如果输入数组是逆序排列的，将出现最坏情况。平均情况与最坏情况一样，其时间代价是(n2)。
<div align=center>

![1589094923219.png](..\images\1589094923219.png)

</div>

 
     public void sort(int arr[]) 
        { 
            for(int i =1; i<arr.length;i++) 
            { 
                //插入的数                 int insertVal = arr[i]; 
                //被插入的位置(准备和前一个数比较) 
                int index = i-1; 
                        //如果插入的数比被插入的数小 
                        while(index>=0&&insertVal<arr[index]) 
                        { 
                            //将把 arr[index] 向后移动                             arr[index+1]=arr[index]; 
                            //让 index 向前移动 
                            index--; 
                        } 
                        //把插入的数放入合适位置 
                arr[index+1]=insertVal; 
            } 
  } 
21.1.3.	快速排序算法 
快速排序的原理：选择一个关键值作为基准值。比基准值小的都在左边序列（一般是无序的），比基准值大的都在右边（一般是无序的）。一般选择序列的第一个元素。 
一次循环：从后往前比较，用基准值和最后一个值比较，如果比基准值小的交换位置，如果没有继续比较下一个，直到找到第一个比基准值小的值才交换。找到这个值之后，又从前往后开始比较，如果有比基准值大的，交换位置，如果没有继续比较下一个，直到找到第一个比基准值大的值才交换。直到从前往后的比较索引>从后往前比较的索引，结束第一次循环，此时，对于基准值来说，左右两边就是有序的了。 
       public void sort(int[] a,int low,int high){          int start = low;          int end = high; 
         int key = a[low];                  while(end>start){              //从后往前比较              while(end>start&&a[end]>=key)  
 //如果没有比关键值小的，比较下一个，直到有比关键值小的交换位置，然后又从前往后比较 
                 end--;              if(a[end]<=key){                  int temp = a[end];                  a[end] = a[start];                  a[start] = temp; 
             } 
             //从前往后比较              while(end>start&&a[start]<=key) 
//如果没有比关键值大的，比较下一个，直到有比关键值大的交换位置 
                start++;              if(a[start]>=key){                  int temp = a[start];                  a[start] = a[end];                  a[end] = temp; 
             } 
         //此时第一次循环比较结束，关键值的位置已经确定了。左边的值都比关键值小，右边的值都比关键值大，但是两边的顺序还有可能是不一样的，进行下面的递归调用 
         } 
         //递归 
         if(start>low) sort(a,low,start-1);//左边序列。第一个索引位置到关键值索引-1          if(end<high) sort(a,end+1,high);//右边序列。从关键值索引+1 到最后一个 
     } 
   } 
 
<div align=center>

![1589094972624.png](..\images\1589094972624.png)

</div>

 
21.1.1. 希尔排序算法 
基本思想：先将整个待排序的记录序列分割成为若干子序列分别进行直接插入排序，待整个序列中的记录“基本有序”时，再对全体记录进行依次直接插入排序。 
1.	操作方法： 
选择一个增量序列 t1，t2，…，tk，其中 ti>tj，tk=1； 
2.	按增量序列个数 k，对序列进行 k 趟排序； 
3.	每趟排序，根据对应的增量 ti，将待排序列分割成若干长度为 m 的子序列，分别对各子表进行直接插入排序。仅增量因子为1 时，整个序列作为一个表来处理，表长度即为整个序列的长度。 
<div align=center>

![1589095001150.png](..\images\1589095001150.png)

</div>

   private void shellSort(int[] a) { 
 	 	 int dk = a.length/2;   	 	 while( dk >= 1  ){   
 	               ShellInsertSort(a, dk);    	                dk = dk/2; 
 	 	 } 
 	} 
 	private void ShellInsertSort(int[] a, int dk) { 
//类似插入排序，只是插入排序增量是 1，这里增量是 dk,把 1 换成 dk 就可以了 
 	 	for(int i=dk;i<a.length;i++){ 
 	 	 	if(a[i]<a[i-dk]){ 
 	 	 	 	int j; 
 	 	 	 	int x=a[i];//x 为待插入元素  	 	 	 	a[i]=a[i-dk]; 
 	 	 	 	for(j=i-dk;  j>=0 && x<a[j];j=j-dk){ 
//通过循环，逐个后移一位找到要插入的位置。 
 	 	 	 	 	a[j+dk]=a[j]; 
 	 	 	 	} 
 	 	 	 	a[j+dk]=x;//插入 
 	 	 	} 
 	 	} 
 	} 
 
21.1.2. 归并排序算法 
归并（Merge）排序法是将两个（或两个以上）有序表合并成一个新的有序表，即把待排序序列分为若干个子序列，每个子序列是有序的。然后再把有序子序列合并为整体有序序列。 
<div align=center>

![1589095032308.png](..\images\1589095032308.png)

</div>


 
 
 
  public class MergeSortTest {        public static void main(String[] args) {           int[] data = new int[] { 5, 3, 6, 2, 1, 9, 4, 8, 7 };           print(data);           mergeSort(data);   
        System.out.println("排序后的数组：");   
        print(data);   
    }   
     public static void mergeSort(int[] data) {           sort(data, 0, data.length - 1);   
    }   
      public static void sort(int[] data, int left, int right) {           if (left >= right)               return;   
        // 找出中间索引   
        int center = (left + right) / 2;   
        // 对左边数组进行递归   
        sort(data, left, center);   
        // 对右边数组进行递归   
        sort(data, center + 1, right);   
        // 合并           merge(data, left, center, right);           print(data);   
    }   
      /**  
     * 将两个数组进行归并，归并前面 2 个数组已有序，归并后依然有序  

     *   
*	@param data  
*	数组对象  
*	@param left  
*	左数组的第一个元素的索引  
*	@param center  
*	左数组的最后一个元素的索引，center+1 是右数组第一个元素的索引  
*	@param right  
*	右数组最后一个元素的索引  
     */   
    public static void merge(int[] data, int left, int center, int right) {   
        // 临时数组           int[] tmpArr = new int[data.length];   
        // 右数组第一个元素索引   
        int mid = center + 1;   
        // third 记录临时数组的索引   
        int third = left;   
        // 缓存左数组第一个元素的索引   
        int tmp = left;           while (left <= center && mid <= right) {   
            // 从两个数组中取出最小的放入临时数组   
            if (data[left] <= data[mid]) {                   tmpArr[third++] = data[left++];   
            } else {   
                tmpArr[third++] = data[mid++];   
            }   
        }   
        // 剩余部分依次放入临时数组（实际上两个 while 只会执行其中一个）   
        while (mid <= right) {               tmpArr[third++] = data[mid++];   
        }   
        while (left <= center) {               tmpArr[third++] = data[left++];   
        }   
        // 将临时数组中的内容拷贝回原数组中   
        // （原 left-right 范围的内容被复制回原数组）   
        while (tmp <= right) {               data[tmp] = tmpArr[tmp++];   
        }   
    }   
    public static void print(int[] data) {           for (int i = 0; i < data.length; i++) {   
            System.out.print(data[i] + "\t");   
        }   
        System.out.println();   
    }   
  }   
 
21.1.3. 桶排序算法 
桶排序的基本思想是： 把数组 arr 划分为 n 个大小相同子区间（桶），每个子区间各自排序，最后合并 。计数排序是桶排序的一种特殊情况，可以把计数排序当成每个桶里只有一个元素的情况。 
1.找出待排序数组中的最大值 max、最小值 min 
2.我们使用 动态数组 ArrayList 作为桶，桶里放的元素也用 ArrayList 存储。桶的数量为(maxmin)/arr.length+1 
3.遍历数组 arr，计算每个元素 arr[i] 放的桶 
4.每个桶各自排序 
  public static void bucketSort(int[] arr){ 
    
  int max = Integer.MIN_VALUE;   int min = Integer.MAX_VALUE; 
  for(int i = 0; i < arr.length; i++){ 
    max = Math.max(max, arr[i]);     min = Math.min(min, arr[i]); 
  } 
   //创建桶 
  int bucketNum = (max - min) / arr.length + 1; 
  ArrayList<ArrayList<Integer>> bucketArr = new ArrayList<>(bucketNum);   for(int i = 0; i < bucketNum; i++){     bucketArr.add(new ArrayList<Integer>()); 
  } 
     //将每个元素放入桶 
  for(int i = 0; i < arr.length; i++){     int num = (arr[i] - min) / (arr.length);     bucketArr.get(num).add(arr[i]); 
  } 
     //对每个桶进行排序 
  for(int i = 0; i < bucketArr.size(); i++){ 
    Collections.sort(bucketArr.get(i)); 
  } 
 } 
 
21.1.4. 基数排序算法 
将所有待比较数值（正整数）统一为同样的数位长度，数位较短的数前面补零。然后，从最低位开始，依次进行一次排序。这样从最低位排序一直到最高位排序完成以后,数列就变成一个有序序列。 
  public class radixSort {       inta[]={49,38,65,97,76,13,27,49,78,34,12,64,5,4,62,99,98,54,101,56,17,18,23,34,15,35,2
5,53,51};       public radixSort(){          sort(a);   
       for(inti=0;i<a.length;i++){                 System.out.println(a[i]);   
       }   
    }          
    public  void sort(int[] array){     
       //首先确定排序的趟数;     
       int max=array[0];            for(inti=1;i<array.length;i++){                 if(array[i]>max){     
              max=array[i];     
            }     
       }     
       int time=0;            //判断位数;            while(max>0){               max/=10;                time++;     
       }    
        //建立 10 个队列;     
       List<ArrayList> queue=newArrayList<ArrayList>();            for(int i=0;i<10;i++){     
              ArrayList<Integer>queue1=new ArrayList<Integer>();              queue.add(queue1);     
       }     
          //进行 time 次分配和收集;     
       for(int i=0;i<time;i++){                //分配数组元素;               for(intj=0;j<array.length;j++){     
               //得到数字的第 time+1 位数;   
                 int x=array[j]%(int)Math.pow(10,i+1)/(int)Math.pow(10, i);                    ArrayList<Integer>queue2=queue.get(x);                    queue2.add(array[j]);                    queue.set(x, queue2);   
          }    
          int count=0;//元素计数器;     
          //收集队列元素;               for(int k=0;k<10;k++){                  while(queue.get(k).size()>0){   
                   ArrayList<Integer>queue3=queue.get(k);                      array[count]=queue3.get(0);                        queue3.remove(0);                      count++;   
               }    
          }     
       }                
    }   
} 
 
21.1.5. 剪枝算法 
在搜索算法中优化中，剪枝，就是通过某种判断，避免一些不必要的遍历过程，形象的说，就是剪去了搜索树中的某些“枝条”，故称剪枝。应用剪枝优化的核心问题是设计剪枝判断方法，即确定哪些枝条应当舍弃，哪些枝条应当保留的方法。 
<div align=center>

![1589095078826.png](..\images\1589095078826.png)

</div>

 
 
21.1.6. 回溯算法 
回溯算法实际上一个类似枚举的搜索尝试过程，主要是在搜索尝试过程中寻找问题的解，当发现已不满足求解条件时，就“回溯”返回，尝试别的路径。 
21.1.7. 最短路径算法 
从某顶点出发，沿图的边到达另一顶点所经过的路径中，各边上权值之和最小的一条路径叫做最短路径。解决最短路的问题有以下算法，Dijkstra 算法，Bellman-Ford 算法，Floyd 算法和 SPFA 算法等。 
21.1.8. 最大子数组算法 
 
21.1.9. 最长公共子序算法 
21.1.10. 最小生成树算法 
现在假设有一个很实际的问题：我们要在 n 个城市中建立一个通信网络，则连通这 n 个城市需要布置 n-1 一条通信线路，这个时候我们需要考虑如何在成本最低的情况下建立这个通信网？  
于是我们就可以引入连通图来解决我们遇到的问题，n 个城市就是图上的 n 个顶点，然后，边表示两个城市的通信线路，每条边上的权重就是我们搭建这条线路所需要的成本，所以现在我们有n个顶点的连通网可以建立不同的生成树，每一颗生成树都可以作为一个通信网，当我们构造这个连通网所花的成本最小时，搭建该连通网的生成树，就称为最小生成树。构造最小生成树有很多算法，但是他们都是利用了最小生成树的同一种性质：MST 性质（假设
N=(V,{E})是一个连通网，U 是顶点集 V 的一个非空子集，如果（u，v）是一条具有最小权值的边，其中 u 属于U，v 属于V-U，则必定存在一颗包含边（u，v）的最小生成树），下面就介绍两种使用 MST 性质生成最小生成树的算法：普里姆算法和克鲁斯卡尔算法。 
<div align=center>

![1589095108032.png](..\images\1589095108032.png)

</div>